{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-09T18:18:20.8667Z","iopub.execute_input":"2021-09-09T18:18:20.867144Z","iopub.status.idle":"2021-09-09T18:18:20.89517Z","shell.execute_reply.started":"2021-09-09T18:18:20.867059Z","shell.execute_reply":"2021-09-09T18:18:20.893464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from transformers import BertTokenizer, BertForQuestionAnswering\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson  was a nice puppet\"\ninputs = tokenizer(question, text, return_tensors='pt')\nstart_positions = torch.tensor([1])\nend_positions = torch.tensor([3])\n\noutputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\nloss = outputs.loss\nstart_scores = outputs.start_logits\nend_scores = outputs.end_logits","metadata":{}},{"cell_type":"markdown","source":"from transformers import ElectraTokenizer\n#tokenizer = ElectraTokenizer.from_pretrained('https://huggingface.co/bert-base-uncased')\ntokenizer = ElectraTokenizer.from_pretrained('hfl/chinese-electra-180g-base-discriminator')\ninputs = tokenizer(\"你来自中国吗\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nlast_hidden_states","metadata":{"execution":{"iopub.status.busy":"2021-09-10T18:47:59.874018Z","iopub.execute_input":"2021-09-10T18:47:59.874421Z","iopub.status.idle":"2021-09-10T18:47:59.932292Z","shell.execute_reply.started":"2021-09-10T18:47:59.874386Z","shell.execute_reply":"2021-09-10T18:47:59.931609Z"}}},{"cell_type":"markdown","source":"all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].numpy()[0])\nanswer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T18:25:37.580809Z","iopub.execute_input":"2021-09-09T18:25:37.581088Z","iopub.status.idle":"2021-09-09T18:25:37.585882Z","shell.execute_reply.started":"2021-09-09T18:25:37.581058Z","shell.execute_reply":"2021-09-09T18:25:37.584855Z"}}},{"cell_type":"code","source":"#answer","metadata":{"execution":{"iopub.status.busy":"2021-09-11T16:49:03.051856Z","iopub.execute_input":"2021-09-11T16:49:03.052261Z","iopub.status.idle":"2021-09-11T16:49:03.059038Z","shell.execute_reply.started":"2021-09-11T16:49:03.052175Z","shell.execute_reply":"2021-09-11T16:49:03.056988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"a,b = \"Who was Jim Henson?\", \"Jim Henson  is not a nice puppet\"\ninputs = tokenizer(a, b, return_tensors='pt')\nstart_positions = torch.tensor([1])\nend_positions = torch.tensor([15])\noutputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\nloss = outputs.loss\nstart_scores = outputs.start_logits\nend_scores = outputs.end_logits","metadata":{"execution":{"iopub.status.busy":"2021-09-09T18:25:32.871385Z","iopub.execute_input":"2021-09-09T18:25:32.872024Z","iopub.status.idle":"2021-09-09T18:25:33.000647Z","shell.execute_reply.started":"2021-09-09T18:25:32.871984Z","shell.execute_reply":"2021-09-09T18:25:32.999823Z"}}},{"cell_type":"markdown","source":"from transformers import BertTokenizer, TFBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertForQuestionAnswering.from_pretrained('bert-base-cased')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\ninput_dict = tokenizer(question, text, return_tensors='tf')\noutputs = model(**input_dict)\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\n\nall_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\nanswer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T18:28:29.950846Z","iopub.execute_input":"2021-09-09T18:28:29.951219Z","iopub.status.idle":"2021-09-09T18:28:35.898644Z","shell.execute_reply.started":"2021-09-09T18:28:29.951189Z","shell.execute_reply":"2021-09-09T18:28:35.897818Z"}}},{"cell_type":"markdown","source":"from transformers import ElectraTokenizer, ElectraModel\nimport torch\n\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\nmodel = ElectraModel.from_pretrained('google/electra-small-discriminator')\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state","metadata":{"execution":{"iopub.status.busy":"2021-09-10T18:43:37.533621Z","iopub.execute_input":"2021-09-10T18:43:37.534024Z","iopub.status.idle":"2021-09-10T18:43:57.529725Z","shell.execute_reply.started":"2021-09-10T18:43:37.533991Z","shell.execute_reply":"2021-09-10T18:43:57.52887Z"}}},{"cell_type":"markdown","source":"# from github baseline","metadata":{}},{"cell_type":"markdown","source":"# import package","metadata":{}},{"cell_type":"code","source":"#必须加gc，垃圾回收机制\nimport gc\n#import logging\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nimport json\nfrom tqdm import tqdm\nfrom transformers import PreTrainedTokenizer, BasicTokenizer\nfrom transformers.tokenization_utils import _is_whitespace, _is_punctuation, _is_control\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, TensorDataset\n#import json\nimport sys\n#import argparse\nfrom transformers import ElectraTokenizer\n#from data_process_utils import * \n#No.1 independence package\nimport gzip\nimport pickle\nimport os\nfrom os.path import join\n#训练\n#import argparse\nfrom transformers import AutoModel, AutoConfig,  AutoTokenizer\n#from model import MultiSpanQA\n#from data_process_utils import *\n#import torch\nimport random\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nimport pickle\nimport gzip\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport logging\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss, BCELoss\nfrom tqdm import trange, tqdm\nimport timeit\n#import os\n#from evaluate_utils import compute_predictions\n#from evaluate_2021 import CJRCEvaluator\n#from os.path import join\n\nfrom collections import Counter, OrderedDict\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:51:37.685756Z","iopub.execute_input":"2021-09-23T15:51:37.686059Z","iopub.status.idle":"2021-09-23T15:51:44.368643Z","shell.execute_reply.started":"2021-09-23T15:51:37.685989Z","shell.execute_reply":"2021-09-23T15:51:44.367799Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data pre pocession utils","metadata":{"execution":{"iopub.status.busy":"2021-09-10T17:34:53.720409Z","iopub.execute_input":"2021-09-10T17:34:53.720856Z","iopub.status.idle":"2021-09-10T17:34:53.725314Z","shell.execute_reply.started":"2021-09-10T17:34:53.720752Z","shell.execute_reply":"2021-09-10T17:34:53.724366Z"}}},{"cell_type":"code","source":"\n\nYES_TOKEN = \"[unused1]\"\nNO_TOKEN = \"[unused2]\"\n\n\nclass CAILExample:\n    def __init__(self,\n                 qas_id: str,\n                 question_text: str,\n                 context_text: str,\n                 answer_texts: List[str],\n                 answer_start_indexes: List[int],\n                 is_impossible: bool,\n                 is_yes_no: bool,\n                 is_multi_span: bool,\n                 answers: List,\n                 case_id: str,\n                 case_name: str):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.context_text = context_text\n        self.answer_texts = answer_texts\n        self.answer_start_indexes = answer_start_indexes\n        self.is_impossible = is_impossible\n        self.is_yes_no = is_yes_no\n        self.is_multi_span = is_multi_span\n        self.answers = answers\n        self.case_id = case_id\n        self.case_name = case_name\n\n        self.doc_tokens = []\n        self.char_to_word_offset = []\n\n        raw_doc_tokens = customize_tokenizer(context_text, True)\n        #转控制与空白与标点符号中文为 空格+字符+空格，+split\n        #其他直接字符\n        k = 0\n        temp_word = \"\"\n        for char in self.context_text:\n            #分行处解决，#question 为什么不直接复制raw_doc_tokens，可能要生成char_to_word_offset\n            #char_to_word_offset（空白的k-1，非空白的k，k在temp_word.lower() == raw_doc_tokens[k]改变）\n            #处理其中的英文字符，数字？\n            if _is_whitespace(char):\n                self.char_to_word_offset.append(k - 1)\n                continue\n            else:\n                temp_word += char\n                self.char_to_word_offset.append(k)\n            if temp_word.lower() == raw_doc_tokens[k]:\n                self.doc_tokens.append(temp_word)#temp_word是单词\n                temp_word = \"\"\n                k += 1\n        assert k == len(raw_doc_tokens)\n\n        if answer_texts is not None:  # if for training\n            start_positions = []\n            end_positions = []\n\n            if not is_impossible and not is_yes_no:\n                for i in range(len(answer_texts)):\n                    answer_offset = context_text.index(answer_texts[i])\n                    answer_length = len(answer_texts[i])\n                    start_position = self.char_to_word_offset[answer_offset]#第几个词里，而非第几个字符里\n                    end_position = self.char_to_word_offset[answer_offset + answer_length - 1]\n                    start_positions.append(start_position)\n                    end_positions.append(end_position)\n            else:\n                start_positions.append(-1)\n                end_positions.append(-1)\n            self.start_positions = start_positions\n            self.end_positions = end_positions\n    #认为是抽取形的答案，因为结束index=开始+答案长度\n    def __repr__(self):\n        string = \"\"\n        for key, value in self.__dict__.items():\n            string += f\"{key}: {value}\\n\"\n        return f\"<{self.__class__}>\"\n\n\n@dataclass\nclass CAILFeature:\n    input_ids: List[int]\n    attention_mask: List[int]\n    token_type_ids: List[int]\n    cls_index: int\n    p_mask: List\n    example_index: int\n    unique_id: int\n    paragraph_len: int\n    token_is_max_context: object\n    tokens: List\n    token_to_orig_map: Dict\n    start_positions: List[int]\n    end_positions: List[int]\n    is_impossible: bool\n\n\n@dataclass\nclass CAILResult:\n    unique_id: int\n    start_logits: torch.Tensor\n    end_logits: torch.Tensor\n\n\ndef read_examples(file: str, is_training: bool) -> List[CAILExample]:\n    example_list = []\n    with open(file, \"r\", encoding=\"utf-8\") as file:\n        original_data = json.load(file)[\"data\"]\n\n    for entry in tqdm(original_data, disable=True):\n        case_id = entry[\"caseid\"]\n        for paragraph in entry[\"paragraphs\"]:\n            context = paragraph[\"context\"]\n            case_name = paragraph[\"casename\"]\n            for qa in paragraph[\"qas\"]:\n                question = qa[\"question\"]\n                qas_id = qa[\"id\"]\n                answer_texts = None\n                answer_starts = None\n                is_impossible = None\n                is_yes_no = None\n                is_multi_span = None\n                all_answers = None\n                if is_training:\n                    all_answers = qa[\"answers\"]\n                    if len(all_answers) == 0:\n                        answer = []\n                    else:\n                        answer = all_answers[0]\n                    # a little difference between 19 and 21 data.\n                    #19年data里是内容，21年是[内容]\n                    #因为19年每个问句中只有一个提问，21年有如此提问：什么时候对a案件做出了什么样的宣判？\n                    if type(answer) == dict:\n                        answer = [answer]\n\n                    if len(answer) == 0:  # NO Answer\n                        answer_texts = [\"\"]\n                        answer_starts = [-1]\n                    else:\n                        answer_texts = []\n                        answer_starts = []\n                        for a in answer:\n                            answer_texts.append(a[\"text\"])\n                            answer_starts.append(a[\"answer_start\"])\n                    # Judge YES or NO\n                    if len(answer_texts) == 1 and answer_starts[0] == -1 and (answer_texts[0] == \"YES\" or answer_texts[0] == \"NO\"):\n                        is_yes_no = True\n                    else:\n                        is_yes_no = False\n                    # Judge Multi Span\n                    if len(answer_texts) > 1:\n                        is_multi_span = True\n                    else:\n                        is_multi_span = False\n                    # Judge No Answer\n                    if len(answer_texts) == 1 and answer_texts[0] == \"\":\n                        is_impossible = True\n                    else:\n                        is_impossible = False\n\n                example = CAILExample(\n                    qas_id=qas_id,\n                    question_text=question,\n                    context_text=context,\n                    answer_texts=answer_texts,\n                    answer_start_indexes=answer_starts,\n                    is_impossible=is_impossible,\n                    is_yes_no=is_yes_no,\n                    is_multi_span=is_multi_span,\n                    answers=all_answers,\n                    case_id=case_id,\n                    case_name=case_name\n                )\n                # Discard possible bad example\n                if is_training and example.answer_start_indexes[0] >= 0:\n                    for i in range(len(example.answer_texts)):\n                        actual_text = \"\".join(example.doc_tokens[example.start_positions[i]: (example.end_positions[i] + 1)])\n                        cleaned_answer_text = \"\".join(whitespace_tokenize(example.answer_texts[i]))#去除空格\n                        if actual_text.find(cleaned_answer_text) == -1:\n                            print(f\"Could not find answer: {actual_text} vs. {cleaned_answer_text}\")\n                            continue\n                example_list.append(example)\n    return example_list\n\n\ndef convert_examples_to_features(example_list: List[CAILExample], tokenizer: PreTrainedTokenizer, args,\n                                 is_training: bool) -> List[CAILFeature]:\n    # Validate there are no duplicate ids in example_list\n    qas_id_set = set()\n    for example in example_list:\n        if example.qas_id in qas_id_set:\n            raise Exception(\"Duplicate qas_id!\")\n        else:\n            qas_id_set.add(example.qas_id)\n\n    feature_list = []\n    unique_id = 0\n    example_index = 0\n    i = 0\n    for example in tqdm(example_list, disable=True):\n        i += 1\n        if i % 1000 == 0:\n            print(i)\n        current_example_features = convert_single_example_to_features(example, tokenizer, args.max_seq_length,\n                                                                      args.max_query_length, args.doc_stride, is_training)\n        for feature in current_example_features:\n            feature.example_index = example_index#对特征所属的example编码\n            feature.unique_id = unique_id\n            unique_id += 1\n        example_index += 1\n        #unique_id 和example_index都是编码\n        feature_list.extend(current_example_features)\n\n    return feature_list\n\n\ndef convert_single_example_to_features(example: CAILExample, tokenizer: PreTrainedTokenizer,\n                                       max_seq_length, max_query_length, doc_stride, is_training) -> List[CAILFeature]:\n    \"\"\"\n    Transfer original text to sequence which can be accepted by ELECTRA\n    Format: [CLS] YES_TOKEN NO_TOKEN question [SEP] context [SEP]\n    \"\"\"\n    features = []\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n    for (i, token) in enumerate(example.doc_tokens):\n        #orig_to_tok_index，token到\n        orig_to_tok_index.append(len(all_doc_tokens))\n        ##if i==1:\n        #    print('info about example token {},{}'.format(type(token),token))\n        #token是单个汉字\n        sub_tokens = tokenizer.tokenize(token)\n        if i==2:\n            print('info about example token {},{}'.format(sub_tokens,token))\n            #好家伙，几乎不进行编码的，只是str变成了list？\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_doc_tokens.append(sub_token)\n    #all_doc_token是保存的tokenizer后的token\n\n    if is_training:\n        if example.is_impossible or example.answer_start_indexes[0] == -1:\n            start_positions = [-1]\n            end_positions = [-1]\n        else:\n            start_positions = []\n            end_positions = []\n            for i in range(len(example.start_positions)):\n                start_position = orig_to_tok_index[example.start_positions[i]]\n                if example.end_positions[i] < len(example.doc_tokens) - 1:\n                    end_position = orig_to_tok_index[example.end_positions[i] + 1] - 1\n                else:\n                    end_position = len(all_doc_tokens) - 1\n                (start_position, end_position) = _improve_answer_span(\n                    all_doc_tokens, start_position, end_position, tokenizer, example.answer_texts[i]\n                )\n                start_positions.append(start_position)#token级别的\n                end_positions.append(end_position)\n    else:\n        start_positions = None\n        end_positions = None\n\n    query_tokens = tokenizer.tokenize(example.question_text)\n    query_tokens = [YES_TOKEN, NO_TOKEN] + query_tokens\n    truncated_query = tokenizer.encode(query_tokens, add_special_tokens=False, max_length=max_query_length, truncation=True)\n    #什么情况\n    #Returns the number of added tokens when encoding a sequence with special tokens.\n    sequence_pair_added_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n    assert sequence_pair_added_tokens == 3\n    #pair：Whether the number of added tokens should be computed in the case of a sequence pair or a single sequence.\n\n\n    added_tokens_num_before_second_sequence = tokenizer.num_special_tokens_to_add(pair=False)\n    assert added_tokens_num_before_second_sequence == 2\n    #pair true  有第二个句子\n    #pair false 第二个句子是None\n    #这两句assert有什么意义？，难道是测试num_special_tokens_to_add和build函数\n    span_doc_tokens = all_doc_tokens\n    spans = []\n    print('The length of all_doc_tokens is {}'.format(len(all_doc_tokens)))\n    order_while=0\n    while len(spans) * doc_stride < len(all_doc_tokens):\n#知识点：encode_plus:This method is deprecated, __call__ should be used instead.\n        order_while+=1\n        encoded_dict = tokenizer.encode_plus(\n            truncated_query,\n            span_doc_tokens,#？ 分别将问题与其他文本编码？\n            max_length=max_seq_length,\n            return_overflowing_tokens=True,\n            padding=\"max_length\",\n            stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n            truncation=\"only_second\",\n            return_token_type_ids=True\n        )\n        #print(encoded_dict.keys())\n        print('The length of encoded_dict is{}'.format(len(encoded_dict['input_ids'])))\n        paragraph_len = min(\n            len(all_doc_tokens) - len(spans) * doc_stride,\n            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n        )\n\n        if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n            non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n        else:\n            non_padded_ids = encoded_dict[\"input_ids\"]\n        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n\n        token_to_orig_map = {}\n        token_to_orig_map[0] = -1\n        token_to_orig_map[1] = -1\n        token_to_orig_map[2] = -1\n\n        token_is_max_context = {0: True, 1: True, 2: True}\n        for i in range(paragraph_len):\n            index = len(truncated_query) + added_tokens_num_before_second_sequence + i\n            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n    #分类编码\n        encoded_dict[\"paragraph_len\"] = paragraph_len\n        encoded_dict[\"tokens\"] = tokens\n        encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n        encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + added_tokens_num_before_second_sequence\n        encoded_dict[\"token_is_max_context\"] = token_is_max_context\n        encoded_dict[\"start\"] = len(spans) * doc_stride\n        encoded_dict[\"length\"] = paragraph_len\n\n        encoded_dict[\"token_type_ids\"][1] = 1\n        encoded_dict[\"token_type_ids\"][2] = 1\n\n        spans.append(encoded_dict)\n        #岂不是全都break？\n        if \"overflowing_tokens\" not in encoded_dict or len(encoded_dict[\"overflowing_tokens\"]) == 0:\n            #加入测试,每次这里都会运行到\n            #**overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length`\n            #is specified and  :obj:`return_overflowing_tokens=True`).\n            #print('overflowing_tokens in encoded result in breaking')\n            break\n        else:\n            #print('PositionNo.1 run')\n            \n            span_doc_tokens = encoded_dict[\"overflowing_tokens\"]\n            print('The length of span_doc{} is {}'.format(order_while,len(span_doc_tokens)))\n            \n\n    for doc_span_index in range(len(spans)):\n        for j in range(spans[doc_span_index][\"paragraph_len\"]):\n            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)\n            index = spans[doc_span_index][\"truncated_query_with_special_tokens_length\"] + j\n            spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n\n    for span in spans:\n        cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n\n        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n        p_mask = np.array(span[\"token_type_ids\"])#0是前一段，即问题，1是后一段即文本\n        p_mask = np.minimum(p_mask, 1)\n        p_mask = 1 - p_mask\n        p_mask[np.where(np.array(span[\"input_ids\"]) == tokenizer.sep_token_id)[0]] = 1\n        p_mask[cls_index] = 0\n        p_mask[1] = 0\n        p_mask[2] = 0\n\n        current_start_positions = None\n        current_end_positions = None\n        span_is_impossible = None\n        if is_training:\n            current_start_positions = [0 for i in range(max_seq_length)]\n            current_end_positions = [0 for i in range(max_seq_length)]\n            doc_start = span[\"start\"]\n            doc_end = span[\"start\"] + span[\"length\"] - 1\n            doc_offset = len(truncated_query) + added_tokens_num_before_second_sequence#num of added tokens+2\n            #context的位置\n            for i in range(len(start_positions)):\n                start_position = start_positions[i]#token position\n                end_position = end_positions[i]\n                if start_position >= doc_start and end_position <= doc_end:#都转到原始文本的token级进行比较\n                    span_is_impossible = False#span内有答案\n                    current_start_positions[start_position - doc_start + doc_offset] = 1\n                    current_end_positions[end_position - doc_start + doc_offset] = 1\n                    #用以区分跨span？\n#19年有许多YES，No问题，以answer_start=-1标记\n#19年的big train data 含有is_impossible='True'的情况，代表空答案\n            if example.is_yes_no:\n                assert len(example.answer_start_indexes) == 1\n                assert 1 not in current_start_positions and 1 not in current_end_positions\n                #知识点:检查数据可以使用assert，表明is_yes_no问题答案不是来自文本的抽取答案\n                #没有进answer的划分且answer_start_indexes只有一个内容\n                if example.answer_texts[0] == \"YES\" and example.answer_start_indexes[0] == -1:\n                    current_start_positions[1] = 1\n                    current_end_positions[1] = 1\n                elif example.answer_texts[0] == \"NO\" and example.answer_start_indexes[0] == -1:\n                    current_start_positions[2] = 1\n                    current_end_positions[2] = 1\n                else:\n                    raise Exception(\"example构造出错,请检查\")\n                span_is_impossible = False\n\n            if 1 not in current_start_positions:  # Current Feature does not contain answer span\n                span_is_impossible = True\n                current_start_positions[cls_index] = 1#把答案位置调节到cls在的index\n                current_end_positions[cls_index] = 1#把答案位置调节到cls在的index\n            assert span_is_impossible is not None\n            #注意如何对跨span答案和yes_no问题区分的\n        features.append(\n            CAILFeature(\n                input_ids=span[\"input_ids\"],\n                attention_mask=span[\"attention_mask\"],\n                token_type_ids=span[\"token_type_ids\"],\n                cls_index=cls_index,\n                p_mask=p_mask.tolist(),\n                example_index=0,#在外部进行编码\n                unique_id=0,#在外部进行编码\n                paragraph_len=span[\"paragraph_len\"],\n                token_is_max_context=span[\"token_is_max_context\"],\n                tokens=span[\"tokens\"],\n                token_to_orig_map=span[\"token_to_orig_map\"],\n                start_positions=current_start_positions,\n                end_positions=current_end_positions,\n                is_impossible=span_is_impossible\n            )\n        )#features里面是数个span，每个span由question，context和答案组成\n    return features\n\n\ndef convert_features_to_dataset(features: List[CAILFeature], is_training: bool) -> Dataset:\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n    all_example_indexes = torch.tensor([f.example_index for f in features], dtype=torch.long)\n    all_feature_indexes = torch.arange(all_input_ids.size(0), dtype=torch.long)\n    if is_training:\n        all_is_impossible = torch.tensor([f.is_impossible for f in features], dtype=torch.float)\n        all_start_labels = torch.tensor([f.start_positions for f in features], dtype=torch.float)\n        all_end_labels = torch.tensor([f.end_positions for f in features], dtype=torch.float)\n        dataset = TensorDataset(\n            all_input_ids,\n            all_attention_masks,\n            all_token_type_ids,\n            all_start_labels,\n            all_end_labels,\n            all_cls_index,\n            all_p_mask,\n            all_is_impossible,\n            all_example_indexes,\n            all_feature_indexes\n        )#dataset是所有的数据组成的数据集\n    else:\n        dataset = TensorDataset(\n            all_input_ids,\n            all_attention_masks,\n            all_token_type_ids,\n            all_cls_index,\n            all_p_mask,\n            all_example_indexes,\n            all_feature_indexes\n        )\n    return dataset\n\n\ndef _is_whitespace(c):\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n        return True\n    return False\n\n\ndef _new_check_is_max_context(doc_spans, cur_span_index, position):\n    \"\"\"\n    Check if this is the 'max context' doc span for the token.\n    \"\"\"\n    # if len(doc_spans) == 1:\n    # return True\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n        if position < doc_span[\"start\"]:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span[\"start\"]\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]#和length有关和position的划分有关\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index#所有的span才返回一个true\n\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n    \"\"\"\n    Returns tokenized answer spans that better match the annotated answer.\n    没太明白,原本答案文本和input_start到input_end之间对应的原始文本不应该本就对应吗,为什么还可能找出一个对应的子区间?...\n    \"\"\"\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\ndef customize_tokenizer(text: str, do_lower_case=True) -> List[str]:\n    temp_x = \"\"\n    for char in text:\n        if _is_chinese_char(ord(char)) or _is_punctuation(char) or _is_whitespace(char) or _is_control(char):\n            temp_x += \" \" + char + \" \"\n        else:\n            temp_x += char\n    if do_lower_case:\n        temp_x = temp_x.lower()\n    return temp_x.split()\n\n\ndef _is_chinese_char(cp):\n    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if (\n            (cp >= 0x4E00 and cp <= 0x9FFF)\n            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n            or (cp >= 0xF900 and cp <= 0xFAFF)\n            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n    ):  #\n        return True\n\n    return False\n\n\ndef whitespace_tokenize(text: str):\n    if text is None:\n        return []\n    text = text.strip()\n    tokens = text.split()\n    return tokens\n\n\ndef write_example_orig_file(examples: List[CAILExample], file: str):\n    \"\"\"\n    convert examples to original json file\n    \"\"\"\n    data_list = []\n    for example in examples:\n        data = {\n            \"paragraphs\": [\n                {\n                    \"context\": example.context_text,\n                    \"casename\": example.case_name,\n                    \"qas\": [\n                        {\n                            \"question\": example.question_text,\n                            \"answers\": example.answers,\n                            \"id\": example.qas_id,\n                            \"is_impossible\": \"true\" if example.is_impossible else \"false\",\n                        }\n                    ]\n                }\n            ],\n            \"caseid\": example.case_id\n        }\n        data_list.append(data)\n    final_data = {\n        \"data\": data_list,\n        \"version\": \"1.0\"\n    }\n    with open(file, mode=\"w\", encoding=\"utf-8\") as file:\n        file.write(json.dumps(final_data, ensure_ascii=False))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:49:45.442639Z","iopub.execute_input":"2021-09-23T16:49:45.442973Z","iopub.status.idle":"2021-09-23T16:49:45.520880Z","shell.execute_reply.started":"2021-09-23T16:49:45.442941Z","shell.execute_reply":"2021-09-23T16:49:45.520004Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#test For Data\nwith open('../input/forcail2021/ydlj_cjrc3.0_small_train.json', \"r\", encoding=\"utf-8\") as file:\n        original_data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T03:00:38.070503Z","iopub.execute_input":"2021-09-11T03:00:38.07086Z","iopub.status.idle":"2021-09-11T03:00:38.240454Z","shell.execute_reply.started":"2021-09-11T03:00:38.070827Z","shell.execute_reply":"2021-09-11T03:00:38.239631Z"}}},{"cell_type":"markdown","source":"type(original_data['data'])","metadata":{"execution":{"iopub.status.busy":"2021-09-11T03:01:03.079674Z","iopub.execute_input":"2021-09-11T03:01:03.080094Z","iopub.status.idle":"2021-09-11T03:01:03.085118Z","shell.execute_reply.started":"2021-09-11T03:01:03.080059Z","shell.execute_reply":"2021-09-11T03:01:03.084392Z"}}},{"cell_type":"code","source":"tokenized_example={'input_ids': [[101, 2129, 2116, 5222, 2515, 1996, 10289, 8214, 2273, 1005, 1055, 3455, 2136, 2031, 1029, 102, 1996, 2273, 1005, 1055, 3455, 2136, 2038, 2058, 1015, 1010, 5174, 5222, 1010, 2028, 1997, 2069, 2260, 2816, 2040, 2031, 2584, 2008, 2928, 1010, 1998, 2031, 2596, 1999, 2654, 5803, 8504, 1012, 2280, 2447, 5899, 12385, 4324, 1996, 2501, 2005, 2087, 2685, 3195, 1999, 1037, 2309, 2208, 1997, 1996, 2977, 2007, 6079, 1012, 2348, 1996, 2136, 2038, 2196, 2180, 1996, 5803, 2977, 1010, 2027, 2020, 2315, 2011, 1996, 16254, 2015, 5188, 3192, 2004, 2120, 3966, 3807, 1012, 1996, 2136, 2038, 23339, 1037, 2193, 1997, 6314, 2015, 1997, 2193, 2028, 4396, 2780, 1010, 1996, 2087, 3862, 1997, 2029, 2001, 4566, 12389, 1005, 1055, 2501, 6070, 1011, 2208, 3045, 9039, 1999, 3326, 1012, 1996, 2136, 2038, 7854, 2019, 3176, 2809, 2193, 1011, 2028, 2780, 1010, 1998, 2216, 3157, 5222, 4635, 2117, 1010, 2000, 12389, 1005, 1055, 2184, 1010, 2035, 1011, 2051, 1999, 5222, 2114, 1996, 2327, 2136, 1012, 1996, 2136, 3248, 1999, 4397, 10601, 26429, 10531, 1006, 2306, 1996, 9493, 1052, 1012, 11830, 2415, 1007, 1010, 2029, 11882, 2005, 1996, 2927, 1997, 1996, 2268, 1516, 2230, 2161, 1012, 1996, 2136, 2003, 8868, 2011, 3505, 7987, 3240, 1010, 2040, 1010, 2004, 1997, 1996, 2297, 1516, 2321, 2161, 1010, 2010, 16249, 2012, 10289, 8214, 1010, 2038, 4719, 1037, 29327, 1011, 13913, 2501, 1012, 1999, 2268, 2027, 2020, 4778, 2000, 1996, 9152, 2102, 1010, 2073, 2027, 3935, 2000, 1996, 8565, 2021, 2020, 7854, 2011, 9502, 2110, 2040, 2253, 2006, 1998, 3786, 23950, 1999, 1996, 2528, 1012, 1996, 2230, 1516, 2340, 2136, 5531, 2049, 3180, 2161, 4396, 2193, 2698, 1999, 1996, 2406, 1010, 2007, 1037, 2501, 1997, 2423, 1516, 1019, 1010, 7987, 3240, 1005, 1055, 3587, 3442, 2322, 1011, 2663, 2161, 1010, 1998, 1037, 2117, 1011, 2173, 3926, 1999, 1996, 2502, 2264, 1012, 2076, 1996, 2297, 1011, 2321, 2161, 1010, 1996, 2136, 2253, 3590, 1011, 1020, 1998, 2180, 1996, 16222, 3034, 2977, 1010, 2101, 10787, 2000, 1996, 7069, 1022, 1010, 2073, 1996, 3554, 3493, 2439, 2006, 1037, 4771, 12610, 2121, 1011, 3786, 2121, 2114, 2059, 15188, 5612, 1012, 2419, 2011, 6452, 4433, 11214, 15333, 6862, 3946, 1998, 6986, 9530, 2532, 18533, 2239, 1010, 1996, 3554, 3493, 3786, 1996, 9523, 2120, 3410, 3804, 2630, 13664, 3807, 2076, 1996, 2161, 1012, 1996, 3590, 5222, 2020, 102], [101, 2129, 2116, 5222, 2515, 1996, 10289, 8214, 2273, 1005, 1055, 3455, 2136, 2031, 1029, 102, 2528, 1012, 1996, 2230, 1516, 2340, 2136, 5531, 2049, 3180, 2161, 4396, 2193, 2698, 1999, 1996, 2406, 1010, 2007, 1037, 2501, 1997, 2423, 1516, 1019, 1010, 7987, 3240, 1005, 1055, 3587, 3442, 2322, 1011, 2663, 2161, 1010, 1998, 1037, 2117, 1011, 2173, 3926, 1999, 1996, 2502, 2264, 1012, 2076, 1996, 2297, 1011, 2321, 2161, 1010, 1996, 2136, 2253, 3590, 1011, 1020, 1998, 2180, 1996, 16222, 3034, 2977, 1010, 2101, 10787, 2000, 1996, 7069, 1022, 1010, 2073, 1996, 3554, 3493, 2439, 2006, 1037, 4771, 12610, 2121, 1011, 3786, 2121, 2114, 2059, 15188, 5612, 1012, 2419, 2011, 6452, 4433, 11214, 15333, 6862, 3946, 1998, 6986, 9530, 2532, 18533, 2239, 1010, 1996, 3554, 3493, 3786, 1996, 9523, 2120, 3410, 3804, 2630, 13664, 3807, 2076, 1996, 2161, 1012, 1996, 3590, 5222, 2020, 1996, 2087, 2011, 1996, 3554, 3493, 2136, 2144, 5316, 1011, 5641, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0]}\ntokenized_example.keys()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T08:31:58.724651Z","iopub.execute_input":"2021-09-23T08:31:58.724997Z","iopub.status.idle":"2021-09-23T08:31:58.886158Z","shell.execute_reply.started":"2021-09-23T08:31:58.724925Z","shell.execute_reply":"2021-09-23T08:31:58.88531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_keys(['overflowing_tokens', 'num_truncated_tokens', 'input_ids', 'token_type_ids', 'attention_mask'])\ndict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:41:46.047599Z","iopub.execute_input":"2021-09-23T16:41:46.047939Z","iopub.status.idle":"2021-09-23T16:41:47.041144Z","shell.execute_reply.started":"2021-09-23T16:41:46.047909Z","shell.execute_reply":"2021-09-23T16:41:47.040078Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Data classifacation and some evaluation","metadata":{}},{"cell_type":"code","source":"OPTS = None\n\nTYPE_YES = \"Yes\"\nTYPE_NO = \"No\"\nTYPE_NO_ANSWER = \"No Answer\"\nTYPE_SINGLE_SPAN = \"Single Span\"\nTYPE_MULTI_SPAN = \"Multi Span\"","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:54:09.391006Z","iopub.execute_input":"2021-09-23T15:54:09.391451Z","iopub.status.idle":"2021-09-23T15:54:09.397869Z","shell.execute_reply.started":"2021-09-23T15:54:09.391395Z","shell.execute_reply":"2021-09-23T15:54:09.397077Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n#from data_process_utils import CAILExample, read_examples\n\n\n\n\n\nclass CJRCEvaluator:\n    def __init__(self, gold_file):\n        examples = read_examples(gold_file, True)\n        id_to_example = {}\n        for example in examples:\n            id_to_example[example.qas_id] = example\n        self.id_to_example = id_to_example\n        self.gold_data = CJRCEvaluator.gold_answers_to_dict(gold_file)\n\n    @staticmethod\n    def gold_answers_to_dict(gold_file):\n        dataset = json.load(open(gold_file, mode=\"r\", encoding=\"utf-8\"))\n        gold_dict = {}\n        # id_to_domain = {}\n        for story in dataset['data']:\n            qas = story[\"paragraphs\"][0][\"qas\"]\n            for qa in qas:\n                qid = qa['id']\n                gold_answers = []\n                answers = qa[\"answers\"]\n                if len(answers) == 0:\n                    gold_answers = ['']\n                else:\n                    for answer in qa[\"answers\"]:\n                        if type(answer) == dict:\n                            gold_answers.append(answer[\"text\"])\n                        elif type(answer) == list:\n                            gold_answers.append(\"\".join([a[\"text\"] for a in answer]))\n                if qid in gold_dict:\n                    sys.stderr.write(\"Gold file has duplicate stories: {}\".format(qid))\n                gold_dict[qid] = gold_answers\n        return gold_dict\n\n    @staticmethod\n    def preds_to_dict(pred_file):\n        preds = json.load(open(pred_file, mode=\"r\", encoding=\"utf-8\"))\n        pred_dict = {}\n        for pred in preds:\n            pred_dict[pred['id']] = \"\".join(pred['answer'])\n        return pred_dict\n\n    @staticmethod\n    def normalize_answer(s):\n        \"\"\"Lower text and remove punctuation, storys and extra whitespace.\"\"\"\n\n        def remove_punc(text):\n            return \"\".join(ch for ch in text if ch.isdigit() or ch.isalpha())\n\n        def lower(text):\n            return text.lower()\n    \n        return remove_punc(lower(s))\n\n    @staticmethod\n    def get_tokens(s):\n        if not s: return []\n        return list(CJRCEvaluator.normalize_answer(s))\n\n    @staticmethod\n    def compute_exact(a_gold, a_pred):\n        return int(CJRCEvaluator.normalize_answer(a_gold) == CJRCEvaluator.normalize_answer(a_pred))\n\n    @staticmethod\n    def compute_f1(a_gold, a_pred):\n        gold_toks = CJRCEvaluator.get_tokens(a_gold)\n        pred_toks = CJRCEvaluator.get_tokens(a_pred)\n        common = Counter(gold_toks) & Counter(pred_toks)\n        num_same = sum(common.values())\n        if len(gold_toks) == 0 or len(pred_toks) == 0:\n            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n            return int(gold_toks == pred_toks)\n        if num_same == 0:\n            return 0\n        precision = 1.0 * num_same / len(pred_toks)\n        recall = 1.0 * num_same / len(gold_toks)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1\n\n    @staticmethod\n    def _compute_turn_score(a_gold_list, a_pred):\n        f1_sum = 0.0\n        em_sum = 0.0\n        if len(a_gold_list) > 1:\n            for i in range(len(a_gold_list)):\n                # exclude the current answer\n                gold_answers = a_gold_list[0:i] + a_gold_list[i + 1:]\n                em_sum += max(CJRCEvaluator.compute_exact(a, a_pred) for a in gold_answers)\n                f1_sum += max(CJRCEvaluator.compute_f1(a, a_pred) for a in gold_answers)\n        else:\n            em_sum += max(CJRCEvaluator.compute_exact(a, a_pred) for a in a_gold_list)\n            f1_sum += max(CJRCEvaluator.compute_f1(a, a_pred) for a in a_gold_list)\n        if f1_sum != 1:\n            a = 1 + 1\n        return {'em': em_sum / max(1, len(a_gold_list)), 'f1': f1_sum / max(1, len(a_gold_list))}\n\n    @staticmethod\n    def _compute_turn_score1(a_gold_list, a_pred):\n        f1 = 0.0\n        em = 0.0\n        if len(a_gold_list) > 1:\n            for i in range(len(a_gold_list)):\n                # exclude the current answer\n                em = max(em, CJRCEvaluator.compute_exact(a_gold_list[i], a_pred))\n                f1 = max(f1, CJRCEvaluator.compute_f1(a_gold_list[i], a_pred))\n        else:\n            em = max(CJRCEvaluator.compute_exact(a, a_pred) for a in a_gold_list)\n            f1 = max(CJRCEvaluator.compute_f1(a, a_pred) for a in a_gold_list)\n        if em != 1 or f1 != 1:\n            a = 1 + 1\n        return {'em': em, 'f1': f1}\n\n    def compute_turn_score(self, qid, a_pred):\n        ''' This is the function what you are probably looking for. a_pred is the answer string your model predicted. '''\n        a_gold_list = self.gold_data[qid]\n        return CJRCEvaluator._compute_turn_score(a_gold_list, a_pred)\n\n    def get_raw_scores(self, pred_data):\n        ''''Returns a dict with score'''\n        exact_scores = {}\n        f1_scores = {}\n        for qid in self.gold_data:\n            if qid not in pred_data:\n                sys.stderr.write('Missing prediction for {}\\n'.format(qid))\n                continue\n            a_pred = pred_data[qid]\n            scores = self.compute_turn_score(qid, a_pred)\n            # Take max over all gold answers\n            exact_scores[qid] = scores['em']\n            f1_scores[qid] = scores['f1']\n        return exact_scores, f1_scores\n\n    def get_raw_scores_human(self):\n        '''\n        Returns a dict with score\n        '''\n        exact_scores = {}\n        f1_scores = {}\n        for qid in self.gold_data:\n            f1_sum = 0.0\n            em_sum = 0.0\n            if len(self.gold_data[qid]) > 1:\n                for i in range(len(self.gold_data[qid])):\n                    # exclude the current answer\n                    gold_answers = self.gold_data[qid][0:i] + self.gold_data[qid][i + 1:]\n                    em_sum += max(CJRCEvaluator.compute_exact(a, self.gold_data[qid][i]) for a in gold_answers)\n                    f1_sum += max(CJRCEvaluator.compute_f1(a, self.gold_data[qid][i]) for a in gold_answers)\n            else:\n                exit(\"Gold answers should be multiple: {}={}\".format(qid, self.gold_data[qid]))\n            exact_scores[qid] = em_sum / len(self.gold_data[qid])\n            f1_scores[qid] = f1_sum / len(self.gold_data[qid])\n        return exact_scores, f1_scores\n\n    def human_performance(self):\n        exact_scores, f1_scores = self.get_raw_scores_human()\n        return self.get_total_scores(exact_scores, f1_scores)\n\n    def model_performance(self, pred_data):\n        exact_scores, f1_scores = self.get_raw_scores(pred_data)\n        return self.get_total_scores(exact_scores, f1_scores)\n        # return self.get_qa_types_score(exact_scores, f1_scores)\n\n    def get_total_scores(self, exact_scores, f1_scores):\n        em_total, f1_total, turn_count = 0, 0, 0\n        scores = {}\n        for qid in self.gold_data:\n            em_total += exact_scores.get(qid, 0)\n            f1_total += f1_scores.get(qid, 0)\n            turn_count += 1\n        scores[\"overall\"] = {'em': round(em_total / max(1, turn_count) * 100, 1),\n                             'f1': round(f1_total / max(1, turn_count) * 100, 1),\n                             'qas': turn_count}\n        return scores\n\n    def get_qa_types_score(self, exact_scores, f1_scores):\n        qa_types = {}\n        em_total = 0.0\n        f1_total = 0.0\n        total_count = 0\n        for qid in self.gold_data:\n            type = get_example_qa_type(self.id_to_example[qid])\n            em_total += exact_scores.get(qid, 0)\n            f1_total += f1_scores.get(qid, 0)\n            total_count += 1\n            if type not in qa_types:\n                qa_types[type] = {}\n                qa_types[type][\"em_total\"] = exact_scores.get(qid, 0)\n                qa_types[type]['f1_total'] = f1_scores.get(qid, 0)\n                qa_types[type]['qa_count'] = 1\n            else:\n                qa_types[type][\"em_total\"] += exact_scores.get(qid, 0)\n                qa_types[type]['f1_total'] += f1_scores.get(qid, 0)\n                qa_types[type]['qa_count'] += 1\n        scores = OrderedDict()\n        for type in qa_types.keys():\n            scores[type] = {\n                \"em\": round(qa_types[type][\"em_total\"] / max(1, qa_types[type]['qa_count']) * 100, 1),\n                \"f1\": round(qa_types[type]['f1_total'] / max(1, qa_types[type]['qa_count']) * 100, 1),\n                \"qas\": qa_types[type]['qa_count']\n            }\n        scores[\"F1\"] = round(f1_total / max(1, total_count) * 100, 1)\n        return scores\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:34:33.982463Z","iopub.execute_input":"2021-09-23T15:34:33.982828Z","iopub.status.idle":"2021-09-23T15:34:34.038445Z","shell.execute_reply.started":"2021-09-23T15:34:33.982797Z","shell.execute_reply":"2021-09-23T15:34:34.037458Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_example_qa_type(example: CAILExample):\n    if example.is_impossible:\n        return TYPE_NO_ANSWER\n    if example.is_yes_no:\n        if example.answer_texts[0] == \"YES\":\n            return TYPE_YES\n        elif example.answer_texts[0] == \"NO\":\n            return TYPE_NO\n        else:\n            raise Exception()\n    if len(example.answer_texts) == 1:\n        return TYPE_SINGLE_SPAN\n    if len(example.answer_texts) > 1:\n        return TYPE_MULTI_SPAN\n    raise Exception()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:52:10.346379Z","iopub.execute_input":"2021-09-23T15:52:10.346731Z","iopub.status.idle":"2021-09-23T15:52:10.354543Z","shell.execute_reply.started":"2021-09-23T15:52:10.346701Z","shell.execute_reply":"2021-09-23T15:52:10.353425Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# data generation config and run","metadata":{}},{"cell_type":"code","source":"class ConfigAboutDataSplit:\n    train_21 ='../input/forcail2021/ydlj_cjrc3.0_small_train.json'\n    dev_19='../input/forcail2019/dev_ground_truth.json'\n    train_19='../input/forcail2019/big_train_data.json'\n    train_output = 'train.json'\n    dev_output ='dev.json'\nargs_split=ConfigAboutDataSplit()\n#","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:52:15.384870Z","iopub.execute_input":"2021-09-23T15:52:15.385198Z","iopub.status.idle":"2021-09-23T15:52:15.389148Z","shell.execute_reply.started":"2021-09-23T15:52:15.385168Z","shell.execute_reply":"2021-09-23T15:52:15.388249Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n用于对数据划分，使用19年和21年训练集（8/9）得到新的训练集，使用19年开发集和21年训练集（1/9）得到新的开发集\n\"\"\"\n#from data_process_utils import write_example_orig_file\n#from evaluate_2021 import *\n\n\n\ndef main_data_split(args):\n    '''\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--train_21\",\n        type=str,\n        required=True,\n        help=\"The train set of CAIL 2021.\"\n    )\n    parser.add_argument(\n        \"--dev_19\",\n        type=str,\n        required=True,\n        help=\"The dev set of CAIL 2019.\"\n    )\n    parser.add_argument(\n        \"--train_19\",\n        type=str,\n        required=True,\n        help=\"The train set of CAIL 2019.\"\n    )\n    parser.add_argument(\n        \"--train_output\",\n        type=str,\n        default=\"data/train.json\",\n        help=\"Directory to write train set after split.\"\n    )\n    parser.add_argument(\n        \"--dev_output\",\n        type=str,\n        default=\"data/dev.json\",\n        help=\"Directory to write dev set after split.\"\n    )\n    args = parser.parse_args()\n\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s: %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logging.info(\"All input parameters:\")\n    print(json.dumps(vars(args), sort_keys=False, indent=2))\n    '''\n    # if not os.path.exists(args.output_path):\n    #     os.makedirs(args.output_path)\n\n    examples_train_21 = read_examples(args.train_21, is_training=True)\n    examples_dev_19 = read_examples(args.dev_19, is_training=True)\n    examples_train_19 = read_examples(args.train_19, is_training=True)\n\n    random.shuffle(examples_train_21)\n    random.shuffle(examples_dev_19)\n\n    # split CAIL 2021 train-set to 8 : 1\n    num_per_qa_dev = int(1.0 * len(examples_train_21) * (1 / 9))\n    logging.info(f\"num per qa type in dev: {num_per_qa_dev}\")\n\n    train_set = []\n    dev_set = []\n\n    dev_set.extend(examples_train_21[:num_per_qa_dev])\n    train_set.extend(examples_train_21[num_per_qa_dev:])\n    train_set.extend(examples_train_19[:12*len(examples_train_21)])\n\n    yes_examples_19 = [e for e in examples_dev_19 if get_example_qa_type(e) == TYPE_YES]\n    no_examples_19 = [e for e in examples_dev_19 if get_example_qa_type(e) == TYPE_NO]\n    single_span_examples_19 = [e for e in examples_dev_19 if get_example_qa_type(e) == TYPE_SINGLE_SPAN]\n    no_answer_examples_19 = [e for e in examples_dev_19 if get_example_qa_type(e) == TYPE_NO_ANSWER]\n\n    dev_set.extend(yes_examples_19[:num_per_qa_dev])\n    dev_set.extend(no_examples_19[:num_per_qa_dev])\n    dev_set.extend(single_span_examples_19[:num_per_qa_dev])\n    dev_set.extend(no_answer_examples_19[:num_per_qa_dev])\n\n    print(f\"Train set example num: {len(train_set)}\")\n    print(f\"Test set example num: {len(dev_set)}\")\n\n    random.shuffle(train_set)\n    random.shuffle(dev_set)\n\n    # re-generate id for example to prevent duplicate id between 19 and 21 data\n    for i, example in enumerate(train_set):\n        example.qas_id = i\n    for i, example in enumerate(dev_set):\n        example.qas_id = i\n\n    # write split data back to file\n    write_example_orig_file(train_set, args.train_output)\n    write_example_orig_file(dev_set, args.dev_output)\n\n    print(\"Complete!\")\n\nmain_data_split(args_split)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:54:18.534429Z","iopub.execute_input":"2021-09-23T15:54:18.534777Z","iopub.status.idle":"2021-09-23T15:55:18.606296Z","shell.execute_reply.started":"2021-09-23T15:54:18.534747Z","shell.execute_reply":"2021-09-23T15:55:18.605492Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:36:10.963544Z","iopub.execute_input":"2021-09-23T15:36:10.963920Z","iopub.status.idle":"2021-09-23T15:36:11.945299Z","shell.execute_reply.started":"2021-09-23T15:36:10.963887Z","shell.execute_reply":"2021-09-23T15:36:11.944083Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# data pocess main","metadata":{}},{"cell_type":"code","source":"\nclass ConfigAboutDataPocess:\n    #input_file='../input/forcail2021/train.json' #2020年的数据集不可以\n    #input_file='../input/forcail2021/ydlj_cjrc3.0_small_train.json'\n    input_file='./train.json'\n    #      --for_training \\\n    #      --output_prefix train \\\n    #      --do_lower_case \\\n    #      --tokenizer_path $electra_path \\\n    max_seq_length=512\n    max_query_length=66\n    doc_stride=128\n    output_path='./pocessed_data/'\n    for_training=True\n    #args.max_query_length += 2  # position for token yes and no\n    #max_query_length += 2\n    output_prefix='train'\n    \nargs_data=ConfigAboutDataPocess()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-23T15:57:50.121073Z","iopub.execute_input":"2021-09-23T15:57:50.121417Z","iopub.status.idle":"2021-09-23T15:57:50.127552Z","shell.execute_reply.started":"2021-09-23T15:57:50.121383Z","shell.execute_reply":"2021-09-23T15:57:50.126508Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n测试使用\n\"\"\"\n\ndef convert_and_write(args, tokenizer: PreTrainedTokenizer, file, examples_fn, features_fn, is_training):\n    logging.info(f\"Reading examples from :{file} ...\")\n    example_list = read_examples(file, is_training=is_training)[:50]\n    #logging.info(f\"Total examples:{len(example_list)}\")\n\n    #logging.info(f\"Start converting examples to features.\")\n    feature_list = convert_examples_to_features(example_list, tokenizer, args, is_training)\n    #logging.info(f\"Total features:{len(feature_list)}\")\n\n    #logging.info(f\"Converting complete, writing examples and features to file.\")\n    with gzip.open(join(args_data.output_path, examples_fn), \"wb\") as file:\n        pickle.dump(example_list, file)\n    with gzip.open(join(args_data.output_path, features_fn), \"wb\") as file:\n        pickle.dump(feature_list, file)\n\n\ndef main_data_prepocess():\n    '''\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--input_file\",\n        type=str,\n        required=True,\n        help=\"The file to be processed.\"\n    )\n\n    parser.add_argument(\n        \"--for_training\",\n        action=\"store_true\",\n        help=\"Process for training or not.\"\n    )\n\n    parser.add_argument(\n        \"--output_prefix\",\n        type=str,\n        required=True,\n        help=\"The prefix of output file's name.\"\n    )\n\n    parser.add_argument(\n        \"--do_lower_case\",\n        action=\"store_true\",\n        help=\"Set this flag if you are using an uncased model.\"\n    )\n\n    parser.add_argument(\n        \"--tokenizer_path\",\n        type=str,\n        required=True,\n        help=\"Path to tokenizer which will be used to tokenize text.(ElectraTokenizer)\"\n    )\n\n    parser.add_argument(\n        \"--max_seq_length\",\n        default=512,\n        type=int,\n        help=\"The maximum total input sequence length after WordPiece tokenization. \"\n             \"Longer will be truncated, and shorter will be padded.\"\n    )\n    parser.add_argument(\n        \"--max_query_length\",\n        default=64,\n        type=int,\n        help=\"The maximum number of tokens for the question. Questions longer will be truncated to the length.\"\n    )\n    parser.add_argument(\n        \"--doc_stride\",\n        default=128,\n        type=int,\n        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\"\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./processed_data/\",\n        type=str,\n        help=\"Output path of the constructed examples and features.\"\n    )\n\n    args = parser.parse_args()\n    '''\n    #####\n    #####重新定义参数\n\n    tokenizer = ElectraTokenizer.from_pretrained('hfl/chinese-electra-180g-base-discriminator')\n\n    if not os.path.exists(args_data.output_path):\n        os.makedirs(args_data.output_path)\n    #生成train文件用\n    convert_and_write(args_data, tokenizer, args_data.input_file, args_data.output_prefix+\"_examples.pkl.gz\",\n                       args_data.output_prefix+\"_features.pkl.gz\", args_data.for_training)\n\n\nmain_data_prepocess()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T16:49:54.650616Z","iopub.execute_input":"2021-09-23T16:49:54.651078Z","iopub.status.idle":"2021-09-23T16:50:21.707696Z","shell.execute_reply.started":"2021-09-23T16:49:54.651035Z","shell.execute_reply":"2021-09-23T16:50:21.706817Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n数据处理相关代码\n\"\"\"\n\ndef convert_and_write(args, tokenizer: PreTrainedTokenizer, file, examples_fn, features_fn, is_training):\n    logging.info(f\"Reading examples from :{file} ...\")\n    example_list = read_examples(file, is_training=is_training)\n    #logging.info(f\"Total examples:{len(example_list)}\")\n\n    #logging.info(f\"Start converting examples to features.\")\n    feature_list = convert_examples_to_features(example_list, tokenizer, args, is_training)\n    #logging.info(f\"Total features:{len(feature_list)}\")\n\n    #logging.info(f\"Converting complete, writing examples and features to file.\")\n    with gzip.open(join(args_data.output_path, examples_fn), \"wb\") as file:\n        pickle.dump(example_list, file)\n    with gzip.open(join(args_data.output_path, features_fn), \"wb\") as file:\n        pickle.dump(feature_list, file)\n\n\ndef main_data_prepocess():\n    '''\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--input_file\",\n        type=str,\n        required=True,\n        help=\"The file to be processed.\"\n    )\n\n    parser.add_argument(\n        \"--for_training\",\n        action=\"store_true\",\n        help=\"Process for training or not.\"\n    )\n\n    parser.add_argument(\n        \"--output_prefix\",\n        type=str,\n        required=True,\n        help=\"The prefix of output file's name.\"\n    )\n\n    parser.add_argument(\n        \"--do_lower_case\",\n        action=\"store_true\",\n        help=\"Set this flag if you are using an uncased model.\"\n    )\n\n    parser.add_argument(\n        \"--tokenizer_path\",\n        type=str,\n        required=True,\n        help=\"Path to tokenizer which will be used to tokenize text.(ElectraTokenizer)\"\n    )\n\n    parser.add_argument(\n        \"--max_seq_length\",\n        default=512,\n        type=int,\n        help=\"The maximum total input sequence length after WordPiece tokenization. \"\n             \"Longer will be truncated, and shorter will be padded.\"\n    )\n    parser.add_argument(\n        \"--max_query_length\",\n        default=64,\n        type=int,\n        help=\"The maximum number of tokens for the question. Questions longer will be truncated to the length.\"\n    )\n    parser.add_argument(\n        \"--doc_stride\",\n        default=128,\n        type=int,\n        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\"\n    )\n\n    parser.add_argument(\n        \"--output_path\",\n        default=\"./processed_data/\",\n        type=str,\n        help=\"Output path of the constructed examples and features.\"\n    )\n\n    args = parser.parse_args()\n    '''\n    #####\n    #####重新定义参数\n\n    tokenizer = ElectraTokenizer.from_pretrained('hfl/chinese-electra-180g-base-discriminator')\n\n    if not os.path.exists(args_data.output_path):\n        os.makedirs(args_data.output_path)\n    #生成train文件用\n    convert_and_write(args_data, tokenizer, args_data.input_file, args_data.output_prefix+\"_examples.pkl.gz\",\n                       args_data.output_prefix+\"_features.pkl.gz\", args_data.for_training)\n\n\nmain_data_prepocess()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-11T16:53:26.775401Z","iopub.execute_input":"2021-09-11T16:53:26.775812Z","iopub.status.idle":"2021-09-11T17:02:16.771924Z","shell.execute_reply.started":"2021-09-11T16:53:26.775781Z","shell.execute_reply":"2021-09-11T17:02:16.768229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    print('文件内容')\n    print(os.listdir(args_data.output_path))\nexcept:\n    print('error before model')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T10:18:49.49569Z","iopub.execute_input":"2021-09-11T10:18:49.496032Z","iopub.status.idle":"2021-09-11T10:18:49.50364Z","shell.execute_reply.started":"2021-09-11T10:18:49.496002Z","shell.execute_reply":"2021-09-11T10:18:49.502788Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model define","metadata":{}},{"cell_type":"code","source":"\n\nclass MultiSpanQA(nn.Module):\n    def __init__(self, pretrain_model):\n        super(MultiSpanQA, self).__init__()\n        self.pretrain_model = pretrain_model\n        # represent start logits and end logits respectively\n        self.qa_outputs = nn.Linear(pretrain_model.config.hidden_size, 2)\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            start_labels=None,  # size: (batch_size, max_seq_length, 1)\n            end_labels=None,\n    ):\n        outputs = self.pretrain_model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n        sequence_output = outputs[0]\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_labels is not None and end_labels is not None:\n            loss_fct = BCELoss(reduction=\"mean\")\n            start_loss = loss_fct(torch.sigmoid(start_logits), start_labels)\n            end_loss = loss_fct(torch.sigmoid(end_logits), end_labels)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2021-09-11T17:02:16.773668Z","iopub.status.idle":"2021-09-11T17:02:16.774854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dont know inner function yet","metadata":{}},{"cell_type":"code","source":"  \nimport collections\nimport json\nimport logging\nimport math\nimport re\nimport string\nfrom transformers import BasicTokenizer\nfrom dataclasses import dataclass\nfrom typing import List\n#from data_process_utils import CAILExample\n\n\n@dataclass\nclass Prediction:\n    \"\"\"\n    用来保存可能的预测结果, feature-result-prediction是一一对应的关系\n    \"\"\"\n    feature_index: int\n    start_index: int  # 预测的对应Sequence中的开始位置\n    end_index: int  # 预测的对应Sequence中的结束位置\n    start_logit: float  # 预测的开始得分\n    end_logit: float  # 预测的截止得分\n    text: str = None  # 对应的在原文中的文本,一开始为None,后面被计算\n    orig_start_index: int = None\n    orig_end_index: int = None\n    final_score: int = None\n\n\ndef compute_predictions(\n        all_examples: List[CAILExample],\n        all_features,\n        all_results,\n        n_best_size,\n        max_answer_length,\n        do_lower_case,\n        output_prediction_file,\n        output_nbest_file,\n        verbose_logging,\n        null_score_diff_threshold,\n        tokenizer,\n        args,\n        multi_span_predict: bool = True\n):\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    all_predictions = []\n    all_nbest_predict = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n        predictions = []\n\n        min_cls_score = 1000000\n        min_cls_score_feature_index = None\n        min_cls_start_logits = 0\n        min_cls_end_logits = 0\n\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n\n            feature_null_score = result.start_logits[0] + result.end_logits[0]\n            if feature_null_score < min_cls_score:\n                min_cls_score = feature_null_score\n                min_cls_score_feature_index = feature_index\n                min_cls_start_logits = result.start_logits[0]\n                min_cls_end_logits = result.end_logits[0]\n\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    predictions.append(\n                        Prediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index],\n                        )\n                    )\n        predictions.append(\n            Prediction(\n                feature_index=min_cls_score_feature_index,\n                start_index=0,\n                end_index=0,\n                start_logit=min_cls_start_logits,\n                end_logit=min_cls_end_logits,\n            )\n        )\n\n        predictions = sorted(predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n        seen_predictions = {}\n        filtered_predictions = []\n\n        for prediction in predictions:\n            if len(filtered_predictions) >= n_best_size:\n                break\n\n            feature = features[prediction.feature_index]\n            if prediction.start_index == 1:\n                final_text = \"YES\"\n            elif prediction.start_index == 2:\n                final_text = \"NO\"\n            elif prediction.start_index == 0:\n                final_text = \"\"\n            else:  # this is a non-null prediction\n                tok_tokens = feature.tokens[prediction.start_index: (prediction.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[prediction.start_index]\n                orig_doc_end = feature.token_to_orig_map[prediction.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start: (orig_doc_end + 1)]\n                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n                # tok_text = \" \".join(tok_tokens)\n                #\n                # # De-tokenize WordPieces that have been split off.\n                tok_text = tok_text.replace(\" ##\", \"\")\n                tok_text = tok_text.replace(\"##\", \"\")\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = \" \".join(tok_text.split())\n                orig_text = \" \".join(orig_tokens)\n\n                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n                prediction.orig_start_index = orig_doc_start\n                prediction.orig_end_index = orig_doc_end\n            if final_text in seen_predictions:\n                continue\n            seen_predictions[final_text] = True\n            prediction.text = final_text\n            filtered_predictions.append(prediction)\n        predictions = filtered_predictions\n        if \"\" not in seen_predictions:\n            predictions.append(\n                Prediction(\n                    feature_index=min_cls_score_feature_index,\n                    start_index=0,\n                    end_index=0,\n                    start_logit=min_cls_start_logits,\n                    end_logit=min_cls_end_logits,\n                )\n            )\n        assert len(predictions) > 0\n        if len(predictions) == 1:\n            predictions.append(Prediction(feature_index=-1, start_index=-1, end_index=-1,\n                                          start_logit=0.0, end_logit=0.0, text=\"empty\"))\n\n        score_normalization(predictions)\n        best_non_null_entry = None\n        for p in predictions:\n            if best_non_null_entry is None and p.text != \"\":\n                best_non_null_entry = p\n                break\n        score_diff = min_cls_start_logits + min_cls_end_logits - best_non_null_entry.start_logit - best_non_null_entry.end_logit\n        predict_answers = []\n        if score_diff > null_score_diff_threshold:\n            predict_answers = [\"\"]\n        else:\n            max_score = best_non_null_entry.final_score\n            if best_non_null_entry.start_index > 2:\n                span_covered = [0 for i in range(len(example.doc_tokens))]\n                for p in predictions:\n                    if p.start_index > 2 and p.final_score > (max_score * args.multi_span_threshold) \\\n                            and 1 not in span_covered[p.orig_start_index: (p.orig_end_index + 1)]:\n                        predict_answers.append(p.text)\n                        span_covered[p.orig_start_index: (p.orig_end_index + 1)] = [1 for i in range(p.orig_start_index, p.orig_end_index + 1)]\n            else:\n                predict_answers = [best_non_null_entry.text]\n        if not multi_span_predict:\n            predict_answers = predict_answers[0:1]\n        all_predictions.append({\"id\": example.qas_id, \"answer\": predict_answers})\n        current_nbest = []\n        for (i, prediction) in enumerate(predictions):\n            output = collections.OrderedDict()\n            output[\"text\"] = prediction.text\n            output[\"start_logit\"] = prediction.start_logit\n            output[\"end_logit\"] = prediction.end_logit\n            current_nbest.append(output)\n        all_nbest_predict[example.qas_id] = current_nbest\n\n    with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n        writer.write(json.dumps(all_predictions, indent=4, ensure_ascii=False))\n\n    with open(output_nbest_file, \"w\", encoding=\"utf-8\") as writer:\n        writer.write(json.dumps(all_nbest_predict, indent=4, ensure_ascii=False))\n\n    return all_predictions\n\n\ndef _get_best_indexes(logits, n_best_size):\n    \"\"\"Get the n-best logits from a list.\"\"\"\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don't want in\n    # our prediction.\n    #\n    # For example, let's say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith's\n    #\n    # We don't want to return `orig_text` because it contains the extra \"'s\".\n    #\n    # We don't want to return `pred_text` because it's already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is \"Steve Smith\".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == \" \":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = \"\".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logging.info(\"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\", orig_ns_text, tok_ns_text)\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logging.info(\"Couldn't map start position\")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logging.info(\"Couldn't map end position\")\n        return orig_text\n\n    output_text = orig_text[orig_start_position: (orig_end_position + 1)]\n    return output_text\n\n\ndef score_normalization(predictions: List[Prediction]):\n    scores = [p.start_logit + p.end_logit for p in predictions]\n    max_score = max(scores)\n    min_score = min(scores)\n    for p in predictions:\n        if (max_score - min_score) == 0:\n            p.final_score = 0\n            continue\n        p.final_score = 1.0 * ((p.start_logit + p.end_logit) - min_score) / (max_score - min_score)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T17:02:16.776987Z","iopub.status.idle":"2021-09-11T17:02:16.777952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"markdown","source":"# might change","metadata":{}},{"cell_type":"code","source":"#配置train\nclass ConfigAboutTrainModel:\n    model_path= 'hfl/chinese-electra-180g-base-discriminator'  #huggingface\n    train_data_dir='./pocessed_data'\n    eval_data_dir='./pocessed_data'\n    ground_truth_file='./dev.json'\n    output_dir= './output/'\n    device = 'cuda'\n    batch_size = 8\n    #    --do_lower_case \\\n    #    --logging_steps 200 \\\n    save_steps = 1000\n    gradient_accumulation_steps = 4\n    num_train_epochs = 5\n    learning_rate = 7e-5\n    max_answer_length = 32\n    max_seq_length = 512\n    max_query_length = 64\n    doc_stride = 128\n    multi_span_threshold = 0.8\n    seed = 42\n    weight_decay=0.0\n    adam_epsilon=1e-8\n    warmup_steps=0\n    max_grad_norm=1.0\n    logging_steps=50\n    save_steps=2000\n    \n    \nargs_train=ConfigAboutTrainModel()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T17:02:16.779688Z","iopub.status.idle":"2021-09-11T17:02:16.780492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#t=MultiSpanQA()\n#type(t)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T10:47:56.098917Z","iopub.execute_input":"2021-09-11T10:47:56.0993Z","iopub.status.idle":"2021-09-11T10:47:56.275922Z","shell.execute_reply.started":"2021-09-11T10:47:56.099267Z","shell.execute_reply":"2021-09-11T10:47:56.274787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\n\ndef load_data(file_path):\n    \"\"\"\n    Used to load CAILExample and CAILFeature objects.\n    \"\"\"\n    return pickle.load(gzip.open(file_path, \"rb\"))\n\n\ndef train(args, train_dataset: TensorDataset, model: nn.Module,\n          tokenizer):\n    args.train_batch_size = args.batch_size\n    train_sampler = RandomSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    print(\"***** Running training *****\")\n    \n    logging.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 1\n    epochs_trained = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\",\n    )\n    #set_seed(args)\n\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n                \"start_labels\": batch[3],\n                \"end_labels\": batch[4],\n            }\n            outputs = model(**inputs)\n            loss = outputs[0]\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            loss.backward()\n            tr_loss += loss.item()\n            logging_loss += loss.item()\n\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n                global_step += 1\n\n                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    logging.info(f\"Current global step: {global_step}, start evaluating!\")\n                    logging.info(f\"average loss of batch: {logging_loss / args.logging_steps}\")\n                    logging_loss = 0\n                    results = evaluate(args, model, tokenizer, prefix=f\"{global_step}-dev\",\n                                       eval_data_dir=args.eval_data_dir, ground_truth_file=args.ground_truth_file,\n                                       multi_span_predict=True)\n                    logging.info(f\"Evaluation result in dev-set at global step {global_step}: {results}\")\n                '''\n                if args.save_steps > 0 and global_step % args.save_steps == 0:\n                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, \"module\") else model\n                    torch.save(model_to_save, os.path.join(output_dir, \"checkpoint.bin\"))\n                    tokenizer.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logging.info(\"Saving model checkpoint to %s\", output_dir)\n                '''\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model, tokenizer, prefix, eval_data_dir: str,\n             ground_truth_file: str = None, multi_span_predict=True):\n    logging.info(f\"Loading data for evaluation from {eval_data_dir}!\")\n    examples = load_data(join(args.eval_data_dir, \"dev_examples.pkl.gz\"))\n    features = load_data(join(args.eval_data_dir, \"dev_features.pkl.gz\"))\n    dataset = convert_features_to_dataset(features, is_training=False)\n    logging.info(\"Complete Loading!\")\n\n    args.eval_batch_size = args.batch_size\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    logging.info(\"***** Running evaluation {} *****\".format(prefix))\n    logging.info(\"  Num examples = %d\", len(dataset))\n    logging.info(\"  Batch size = %d\", args.eval_batch_size)\n\n    all_results = []\n    start_time = timeit.default_timer()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=True):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n            }\n            features_indexes = batch[6]\n            outputs = model(**inputs)\n\n        for i, feature_index in enumerate(features_indexes):\n            eval_feature = features[feature_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output = [to_list(o[i]) for o in outputs]\n\n            start_logits, end_logits = output\n\n            result = CAILResult(\n                unique_id=unique_id,\n                start_logits=start_logits,\n                end_logits=end_logits,\n            )\n            all_results.append(result)\n\n    evalTime = timeit.default_timer() - start_time\n    logging.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n\n    # Compute predictions\n    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n\n    compute_predictions(\n        examples,\n        features,\n        all_results,\n        args.n_best_size,\n        args.max_answer_length,\n        args.do_lower_case,\n        output_prediction_file,\n        output_nbest_file,\n        False,\n        args.null_score_diff_threshold,\n        tokenizer,\n        args,\n        multi_span_predict=multi_span_predict\n    )\n    if ground_truth_file is not None:\n        evaluator = CJRCEvaluator(ground_truth_file)\n        pred_data = CJRCEvaluator.preds_to_dict(output_prediction_file)\n        res = evaluator.model_performance(pred_data)\n        return res\n\n\ndef main_train_model(args):\n    '''\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to Electra model.\"\n    )\n\n    parser.add_argument(\n        \"--train_data_dir\",\n        default=None,\n        required=True,\n        type=str,\n        help=\"The directory which contain the generated train-set examples and features file.\"\n    )\n\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory of checkpoints and predictions.\"\n    )\n\n    parser.add_argument(\n        \"--eval_data_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The directory which contain the generated dev-set examples and features file.\"\n    )\n\n    parser.add_argument(\n        \"--ground_truth_file\",\n        default=None,\n        type=str,\n        help=\"The ground truth file of dev-set.\"\n    )\n\n    parser.add_argument(\n        \"--n_best_size\",\n        default=20,\n        type=int,\n        help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n    )\n\n    parser.add_argument(\"--device\", default=\"cuda\", type=str, help=\"Whether not to use CUDA when available\")\n    parser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training and evaluating.\")\n    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization.\")\n    parser.add_argument(\n        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n    parser.add_argument(\n        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n    )\n    parser.add_argument(\n        \"--max_steps\",\n        default=-1,\n        type=int,\n        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n    )\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n    parser.add_argument(\"--save_steps\", type=int, default=600, help=\"Save checkpoint every X updates steps.\")\n\n    parser.add_argument(\n        \"--max_answer_length\",\n        default=32,\n        type=int,\n        help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n             \"and end predictions are not conditioned on one another.\",\n    )\n    parser.add_argument(\n        \"--null_score_diff_threshold\",\n        type=float,\n        default=0.0,\n        help=\"If null_score - best_non_null is greater than the threshold predict null.\",\n    )\n    parser.add_argument(\n        \"--max_seq_length\",\n        default=512,\n        type=int,\n        help=\"The maximum total input sequence length after WordPiece tokenization. \"\n             \"Longer will be truncated, and shorter will be padded.\"\n    )\n    parser.add_argument(\n        \"--max_query_length\",\n        default=64,\n        type=int,\n        help=\"The maximum number of tokens for the question. Questions longer will be truncated to the length.\"\n    )\n    parser.add_argument(\n        \"--doc_stride\",\n        default=128,\n        type=int,\n        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\"\n    )\n\n    parser.add_argument(\n        \"--multi_span_threshold\",\n        type=float,\n        default=0.8,\n        help=\"Span which score is bigger than (max_span_score * multi_span_threshold) will also be output!\"\n    )\n    '''\n    #args = parser.parse_args()\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n\n    logging.info(\"All input parameters:\")\n    #print(json.dumps(vars(args), sort_keys=False, indent=2))\n\n    #set_seed(args)\n\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n    \n    config = AutoConfig.from_pretrained(args_train.model_path)\n    tokenizer = AutoTokenizer.from_pretrained(args_train.model_path)\n    pretrain_model = AutoModel.from_pretrained(args_train.model_path, config=config)\n    model = MultiSpanQA(pretrain_model)\n\n    model.to(device=args_train.device)\n\n    logging.info(\"Loading pre-processed examples and features!\")\n    train_examples = load_data(args_train.train_data_dir + \"/train_examples.pkl.gz\")\n    train_features = load_data(args_train.train_data_dir + \"/train_features.pkl.gz\")\n    print(\"Complete loading!\")\n\n    logging.info(\"Converting features to pytorch Dataset!\")\n    train_dataset = convert_features_to_dataset(train_features, is_training=True)\n    logging.info(\"Complete converting!\")\n\n    global_step, tr_loss = train(args_train, train_dataset, model, tokenizer)\n    logging.info(\"global_step = %s, average loss = %s\", global_step, tr_loss)\n    logging.info(\"Training Complete!\")\n\n    logging.info(\"Saving model checkpoint to %s\", args.output_dir)\n    model_to_save = model.module if hasattr(model, \"module\") else model\n    torch.save(model_to_save, os.path.join(args_train.output_dir, \"checkpoint.bin\"))\n    torch.save(args_train, os.path.join(args_train.output_dir, \"training_args.bin\"))\n    tokenizer.save_pretrained(args_train.output_dir)\n    results = evaluate(args_train, model, tokenizer, prefix=f\"final-eval\",\n                       eval_data_dir=args.eval_data_dir, ground_truth_file=args.ground_truth_file,\n                       multi_span_predict=True)\n    logging.info(f\"Final evaluate results on dev-set: {results}\")\n\n\n#if __name__ == \"__main__\":\nmain_train_model(args_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T17:02:16.782898Z","iopub.status.idle":"2021-09-11T17:02:16.783736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    print(os.listdir( './pocessed_data/'))\nexcept:\n    print('error last step')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T10:49:34.066644Z","iopub.execute_input":"2021-09-11T10:49:34.067097Z","iopub.status.idle":"2021-09-11T10:49:34.109523Z","shell.execute_reply.started":"2021-09-11T10:49:34.067055Z","shell.execute_reply":"2021-09-11T10:49:34.108246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}